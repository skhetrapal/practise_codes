{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiqqJsTZBfZ/f91Zas29RD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skhetrapal/practise_codes/blob/main/NN_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Networks using python basic libraries**"
      ],
      "metadata": {
        "id": "baRuwwgyANc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The following functions are the building blocks to write NN"
      ],
      "metadata": {
        "id": "W7Ke5n9DOmf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzA_2TzfAFDa"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sigmoid function\n",
        "def sigmoid(x):\n",
        "  s = 1/(1+ np.exp(-x))\n",
        "  return s\n",
        "\n",
        "#sigmoid derivative\n",
        "def sigmoid_deriv(x):\n",
        "  s = sigmoid(x)\n",
        "  ds = s*(1-s)\n",
        "  return ds"
      ],
      "metadata": {
        "id": "VjXAIREILxJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_x = np.array([1, 2, 3])\n",
        "print(sigmoid(t_x))\n",
        "print(sigmoid_deriv(t_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEctB5H_NSMd",
        "outputId": "2797fe5c-9b2a-4e97-dfe5-dffe524a044e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.73105858 0.88079708 0.95257413]\n",
            "[0.19661193 0.10499359 0.04517666]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reshaping array to a vector\n",
        "## images have shape: (length, height, 3), the last dimension stores RGB values for each image cell\n",
        "def image2vec(image):\n",
        "  v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1) #coulf also use .reshape(-1,1)\n",
        "  return v"
      ],
      "metadata": {
        "id": "TW9xQl3qMUJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten array to a vector\n",
        "# given test set of number of examples m_train =209, training set of images is of shape (209,64,64,3)\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T #shape = (12288, 209)\n",
        "#train_set_x_orig.reshape(train_set_x_orig.shape[0],-1) gives array of shape (it's transpose gives the final result above)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "3B4733WxdNp8",
        "outputId": "df0435f3-7eae-47b8-8a35-528fbcb7908e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_set_x_orig' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-df0bd8200d0d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# flatten array to a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# given test set of number of examples m_train =209, training set of images is of shape (209,64,64,3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set_x_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m#shape = (12288, 209)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#train_set_x_orig.reshape(train_set_x_orig.shape[0],-1) gives array of shape (it's transpose gives the final result above)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_set_x_orig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_image = np.array([[[ 0.67826139,  0.29380381],\n",
        "                     [ 0.90714982,  0.52835647],\n",
        "                     [ 0.4215251 ,  0.45017551]],\n",
        "\n",
        "                   [[ 0.92814219,  0.96677647],\n",
        "                    [ 0.85304703,  0.52351845],\n",
        "                    [ 0.19981397,  0.27417313]],\n",
        "\n",
        "                   [[ 0.60659855,  0.00533165],\n",
        "                    [ 0.10820313,  0.49978937],\n",
        "                    [ 0.34144279,  0.94630077]]])\n",
        "\n",
        "print(image2vec(t_image))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEr12p_dNa9F",
        "outputId": "37fcf6f7-a68e-4a82-8117-82da04ddf5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.67826139]\n",
            " [0.29380381]\n",
            " [0.90714982]\n",
            " [0.52835647]\n",
            " [0.4215251 ]\n",
            " [0.45017551]\n",
            " [0.92814219]\n",
            " [0.96677647]\n",
            " [0.85304703]\n",
            " [0.52351845]\n",
            " [0.19981397]\n",
            " [0.27417313]\n",
            " [0.60659855]\n",
            " [0.00533165]\n",
            " [0.10820313]\n",
            " [0.49978937]\n",
            " [0.34144279]\n",
            " [0.94630077]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalising rows of a matrix\n",
        "def normalize_rows(x):\n",
        "  x_norm = np.linalg.norm(x,axis=1,keepdims=True) #for columns use axis = 0\n",
        "  x = x/x_norm\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "jNePVxAUOE_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[0, 3, 4],\n",
        "              [1, 6, 4]])\n",
        "print(normalize_rows(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXafhGB9ObOz",
        "outputId": "c97be393-f8d7-4d86-b92f-f9d60af45ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.6        0.8       ]\n",
            " [0.13736056 0.82416338 0.54944226]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#softmax function\n",
        "def softmax(x):\n",
        "  x_exp = np.exp(x)\n",
        "  x_exp_sum = np.linalg.norm(x_exp, axis=1,keepdims=True)\n",
        "  s = x_exp/x_exp_sum\n",
        "  return s"
      ],
      "metadata": {
        "id": "jjY5-7GGOfbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_x = np.array([[9, 2, 5, 0, 0],\n",
        "                [7, 5, 0, 0 ,0]])\n",
        "print(softmax(t_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9U1Hx_dQze0",
        "outputId": "6e71b0fb-99b1-4a1c-c318-72e50af02dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.99831880e-01 9.11728660e-04 1.83125597e-02 1.23389056e-04\n",
            "  1.23389056e-04]\n",
            " [9.90964875e-01 1.34112512e-01 9.03642998e-04 9.03642998e-04\n",
            "  9.03642998e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "##vectorised dot product\n",
        "dot = np.dot(x1,x2) #np.matmul for matrices\n",
        "print(dot)\n",
        "\n",
        "##vectorised outer product\n",
        "outer = np.outer(x1,x2)\n",
        "print(outer)\n",
        "\n",
        "##vectorised elementwise multipliclation\n",
        "elementwise = np.multiply(x1,x2)\n",
        "print(elementwise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgfpIXfbQ1xU",
        "outputId": "3e53956d-7af0-4c65-e6c7-10f0cec62974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "278\n",
            "[[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
            " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
            " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "[81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# L1, L2, cross-entropy loss\n",
        "\n",
        "## L1 loss\n",
        "def L1(yhat, y):\n",
        "  loss = np.sum(np.absolute(y-yhat))\n",
        "  return loss\n",
        "\n",
        "## L2 loss\n",
        "def L2(yhat, y):\n",
        "  loss = np.sum(np.dot(yhat-y,yhat-y))\n",
        "  return loss\n",
        "\n",
        "## cross_entropy_cost\n",
        "def cross_entropy_loss(a,y):\n",
        "  #y is of shape (1,number of examples)\n",
        "  #m = y.shape[1]\n",
        "  loss = -np.sum(np.dot(y,np.log(a).T)+np.dot(1-y,np.log(1-a).T))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "DffolJUdRqWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "\n",
        "print(L1(yhat,y))\n",
        "print(L2(yhat,y))\n",
        "print(cross_entropy_loss(yhat,y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbKL6JJ7Sn0J",
        "outputId": "10f5d303-27d4-4a51-c946-e49ac4eba208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1\n",
            "0.43\n",
            "1.4555158301618436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dcT6Oyicf92a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression as Neural Network"
      ],
      "metadata": {
        "id": "y1zHmyRzUyMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize parameters w, b\n",
        "def initialize(dim):\n",
        "  w = np.rand.randn((dim,1))*0.001 #since w is a vector #\n",
        "  b = 0. #since b is a number"
      ],
      "metadata": {
        "id": "WRkPbLZ7U2_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forward propagation\n",
        "def propagate(w,b,X,Y):\n",
        "  # X is data of size (num_px*num_px*3, number of examples)\n",
        "  m = X.shape[1] #number of examples\n",
        "  A = sigmoid(np.dot(w.T,X)+b)\n",
        "  cost = (-1/m)*cross_entropy_loss(A,Y)\n",
        "  cost = np.squeeze(np.array(cost))\n",
        "\n",
        "  #backprop\n",
        "  dZ = A-Y\n",
        "  dw = (1/m)*np.dot(X,dZ.T)\n",
        "  db = (1/m)*np.sum(dZ)\n",
        "\n",
        "  grads = {\"dw\": dw,\n",
        "           \"db\": db}\n",
        "\n",
        "  return grads,cost\n"
      ],
      "metadata": {
        "id": "Z8VW7IMkfA3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimization function to run the update rule for parameters\n",
        "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
        "  w = copy.deepcopy(w)\n",
        "  b = copy.deepcopy(b)\n",
        "\n",
        "  costs = []\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "    grads, cost = propagate(w,b,X,Y)\n",
        "\n",
        "    dw, db = grads[\"dw\"], grads[\"db\"]\n",
        "\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      costs.append(cost)\n",
        "      # Print the cost every 100 training iterations\n",
        "      if print_cost:\n",
        "        print(\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "  params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "  grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "  return params, grads, costs"
      ],
      "metadata": {
        "id": "Uam3cXNHh7ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict\n",
        "def predict(w, b, X):\n",
        "  m = X.shape[1] #m = number of examples in X, X has shape (num_px*num_px*3,m)\n",
        "  Y_prediction = np.zeros((1, m))\n",
        "  w = w.reshape(X.shape[0], 1) #making sure w is the correct shape\n",
        "\n",
        "  A = sigmoid(np.dot(w.T,X)+b) #shape (1,m)\n",
        "\n",
        "  for i in range(A.shape[1]):\n",
        "    if A[0, i] > 0.5 :\n",
        "             Y_prediction[0,i] = 1\n",
        "    else:\n",
        "             Y_prediction[0,i] = 0\n",
        "\n",
        "  return Y_prediction"
      ],
      "metadata": {
        "id": "qRiTuRdXh970"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#putting the model together\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
        "  w, b = initialize(X_train.shape[0])\n",
        "  params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "  w = params[\"w\"]\n",
        "  b = params[\"b\"]\n",
        "  Y_prediction_test = predict(w, b, X_test)\n",
        "  Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "  d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\" : Y_prediction_train,\n",
        "         \"w\" : w,\n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "  return d"
      ],
      "metadata": {
        "id": "BcG4Od0CkuQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "def load_dataset():\n",
        "    train_dataset = h5py.File('https://github.com/skhetrapal/ML-DL-labs/blob/main/Deep%20Learning/NN_basics/Files_1_2/home/jovyan/work/release/W2A2/datasets/train_catvnoncat.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('https://github.com/skhetrapal/ML-DL-labs/blob/main/Deep%20Learning/NN_basics/Files_1_2/home/jovyan/work/release/W2A2/datasets/test_catvnoncat.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "\n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
        "\n",
        "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
        "\n",
        "train_set_x = train_set_x_flatten / 255.\n",
        "test_set_x = test_set_x_flatten / 255.\n",
        "\n",
        "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "RAcH4Q1PluFz",
        "outputId": "92ec565c-ddd9-462d-9151-3f1f4523c162",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'https://github.com/skhetrapal/ML-DL-labs/blob/main/Deep%20Learning/NN_basics/Files_1_2/home/jovyan/work/release/W2A2/datasets/train_catvnoncat.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-57dba8ff5915>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtrain_set_x_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtest_set_x_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-57dba8ff5915>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://github.com/skhetrapal/ML-DL-labs/blob/main/Deep%20Learning/NN_basics/Files_1_2/home/jovyan/work/release/W2A2/datasets/train_catvnoncat.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_set_x_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_set_x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# your train set features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_set_y_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_set_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# your train set labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    560\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'https://github.com/skhetrapal/ML-DL-labs/blob/main/Deep%20Learning/NN_basics/Files_1_2/home/jovyan/work/release/W2A2/datasets/train_catvnoncat.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch, mini-batch and stochastic gradient descent"
      ],
      "metadata": {
        "id": "MkkaV6FdvQlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#update_parameters_with_gd\n",
        "\n",
        "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
        "  #parameters is a python dictionary parameters['W'+Str(l)] = Wl and similarly for b\n",
        "  L = len(parameters//2) #number of layers in Neural Network\n",
        "\n",
        "  for l in range(1,L+1):\n",
        "    parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads['dW' + str(l)]\n",
        "    parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads['db' + str(l)]\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "li3xm6MfmW1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mini-batch gradient descent with random mini batches\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "  # X has shape (input size, number of examples)\n",
        "  # Y has shape (1, number of examples)\n",
        "  #np.random.seed(seed)           # to fix seed in case repeatability is required\n",
        "  m = X.shape[1]                  # number of training examples\n",
        "  mini_batches = []\n",
        "\n",
        "  # Step 1: Shuffle (X, Y)\n",
        "  permutation = list(np.random.permutation(m))\n",
        "  shuffled_X = X[:, permutation] #keeping 'input size' dimension same, permuting over number of examples\n",
        "  shuffled_Y = Y[:, permutation].reshape((1, m))\n",
        "\n",
        "  inc = mini_batch_size\n",
        "\n",
        "  num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "  for k in range(0, num_complete_minibatches):\n",
        "    mini_batch_X = shuffled_X[:,k * inc : (k+1) * inc]\n",
        "    mini_batch_Y = shuffled_Y[:,k * inc : (k+1) * inc]\n",
        "    mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "    mini_batches.append(mini_batch)\n",
        "\n",
        "  # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
        "  if m % mini_batch_size != 0:\n",
        "    mini_batch_X = shuffled_X[:, num_complete_minibatches * inc : num_complete_minibatches * inc + m - inc * math.floor(m/inc)]\n",
        "    mini_batch_Y = shuffled_Y[:, num_complete_minibatches * inc : num_complete_minibatches * inc + m - inc * math.floor(m/inc)]\n",
        "    mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "    mini_batches.append(mini_batch)\n",
        "\n",
        "  return mini_batches"
      ],
      "metadata": {
        "id": "fwVsJCqMwUF3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}